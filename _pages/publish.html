---
permalink: /publish/
title: "论文著作 [<a href='/fullList/'>Full List</a> ]"
author_profile: true
excerpt: "论文著作"

redirect_from: 
  - /papers.html
---


<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Zhuhua Hu</title>
	<meta content="Zhuhua Hua, huzhuhua.github.io" name="keywords" />
  <!-- 导入外部 CSS 文件 -->
  <!-- <link rel="stylesheet" type="text/css" href="assets\css\custom.css"> -->
  <style>
    body {
        text-align: justify; /* 设置文本两端对齐 */
    }
    .greedy-nav .visible-links li:first-child a {
    margin-left: 30px;
    /* font-size: large; */
}
</style>
	<style media="screen" type="text/css">
  html, body, div, span, applet, object, iframe, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  /* padding: 0pt; */
  vertical-align: baseline;

}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 16pt;
}

b.paper {
  font-weight: bold;
  font-size: 14pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  /* margin: 3em auto 2em auto;
  width: 800px; */
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  background: #eee;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  /* font-size: 13px; */
  font-weight:bold;
}

ul { 
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight: bold;
  color: #FF0000;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.paper div {
  padding-left: 230px;
}

img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>
<style>
  body {
      text-align: justify; /* 设置文本两端对齐 */
  }
</style>
<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<!-- <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45959174-3', 'wanglimin.github.io');
  ga('send', 'pageview');
</script> -->

<body>
<!-- 
<div style="clear: both;">
<div class="section">
<h2>About Me (<a href='wlm_cv.pdf'>CV</a>)</h2>
<div class="paper">
I am a Professor at <a href='http://cs.nju.edu.cn'>Department of Computer Science and Technology</a> and also affiliated with <a href='https://keysoftlab.nju.edu.cn/'>State Key Laboratory for Novel Software Technology</a>, Nanjing University.
<br>
<br>
Previously, I received the B.S. degree from <a href='http://www.nju.edu.cn/'>Nanjing University</a> in 2011, and the Ph.D. degree from <a href='http://www.cuhk.edu.hk/chinese/index.html'>The Chinese University of Hong Kong</a> under the supervision of Prof. <a href='http://www.ie.cuhk.edu.hk/people/xotang.shtml'>Xiaoou Tang</a> in 2015. From 2015 to 2018, I was a Post-Doctoral Researcher with Prof. <a href="http://www.vision.ee.ethz.ch/members/get_member.cgi?lang=en&id=1">Luc Van Gool</a> in the <a href='http://www.vision.ee.ethz.ch/en/'>Computer Vision Laboratory</a> (CVL) at <a href="https://www.ethz.ch/en.html">ETH Zurich</a>. 
</div>
</div>
</div> -->

<!-- 
<div style="clear: both;">
<div class="section">
  <h2>News</h2>
  <div class="paper">
    <ul>
      <li> 2022-03-02: Seven papers on object detection, object tracking, action recognition etc. are accepted by CVPR 2022.</li>
    <li> 2021-07-25: Five papers on video understanding are accepted by ICCV 2021: new dataset (<a href='https://arxiv.org/abs/2105.07404'>MultiSports</a>), backbone (<a href='https://arxiv.org/abs/2005.06803'>TAM</a>), sampling method (<a href='https://arxiv.org/abs/2104.09952'>MGSampler</a>), detection frameworks (<a href='https://arxiv.org/abs/2102.01894'>RTD</a> and <a href='https://arxiv.org/abs/2108.08121'>TRACE</a>). For more details, please refer to our papers. 
    <li> 2021-07-15: <alert>  We release the <a href='https://deeperaction.github.io/multisports/'>MultiSports dataset</a> for spatiotemporal action detection. <alert>  </li> 
    <li> 2021-07-15: Our team secures the first place at <a href='http://auto-video-captions.top/2021/'>ACM MM Pre-training for Video Understanding Challenge</a> for Track 2. </li> 
    <li> 2021-06-15: Our team secures the first place at <a href='https://eval.ai/web/challenges/challenge-page/1054/overview'>CVPR Kinetics Challenge</a> for Self-Supervised Task. </li>
    <li> 2021-06-15: Our team secures the first place at <a href='http://www.picdataset.com/challenge/task/hcvg/'>CVPR PIC Challenge</a> for Human-Centric Spatio-Temporal Video Grounding Task. </li>
    <li> 2021-06-01: <alert> We are organizing <a href='https://deeperaction.github.io'>DeeperAction Challenge</a> at ICCV 2021, by introducing three new benchmarks on temporal action localization, spatiotemporal action detection, and part-level action parsing.  </alert> </li>
    <li> 2021-04-20: The extension of TRecgNet is accpeted by IJCV. </li>
    <li> 2021-04-07: We propose a target transformer for accurate anchor-free tracking, termed as <a href='https://arxiv.org/abs/2104.00403'>TREG</a> (code comming soon). </li>
    <li> 2021-04-07: We present a transformer decoder for direct action proposal generation, termed as <a href='https://arxiv.org/abs/2102.01894'>RTD-Net</a> (code comming soon). </li>	
    <li> 2021-03-01: Two papers on action recognition and point cloud segmentation are accepted by CVPR 2021. </li>	
    <li> 2020-12-30: We propose a new video architecture of using temporal difference, termed as <a href='https://arxiv.org/abs/2012.10071'>TDN</a> and realease the <a href='https://github.com/MCG-NJU/TDN'>code</a>. </li>	
    <li> 2020-07-03: Three papers on action detection and segmentation are accepted by ECCV 2020. </li>	
    <li> 2020-06-28: Our proposed <a href='https://arxiv.org/abs/2006.15560'>DSN</a>, a dynamic version of TSN for efficient action recognition, is accepted by TIP. </li>	
    <li> 2020-05-14: We propose a temporal adaptive module for video recognition, termed as <a href='https://arxiv.org/abs/2005.06803'>TAM</a> and <a href='https://github.com/liu-zhy/TANet'>code</a>. </li>
    <li> 2020-04-16: The code of our published papers will be made available at <a href='https://github.com/MCG-NJU'>Github: MCG-NJU</a>. </li>  
    <li> 2020-04-16: We propose a fully convolutional online tracking framwork, termed as <a href='https://arxiv.org/abs/2004.07109'>FCOT</a> and <a href='https://github.com/MCG-NJU/FCOT'>code</a>. </li>
    <li> 2020-03-10: Our proposed temporal module <a href='https://arxiv.org/abs/2004.01398'>TEA</a> is accepted by CVPR 2020. </li>
    <li> 2020-01-20: We propose an efficient video representation learning framwork, termed as <a href='https://arxiv.org/abs/2001.05691'>CPD</a> and release the <a href='https://github.com/MCG-NJU/CPD-Video'>code</a>. </li>
    <li> 2020-01-15: We present an anchor-free action tubelet detector, termed as <a href='https://arxiv.org/abs/2001.04608'>MOC-Detector</a> and release the <a href='https://github.com/MCG-NJU/MOC-Detector'>code</a>. </li>
    <li> 2019-12-20: Our proposed <a href='https://arxiv.org/abs/2002.07442'> V4D</a>, a principled video-level represenation learning framework, is accepted by ICLR 2020. </li>
    <li> 2019-11-21: Our proposed <a href='https://arxiv.org/abs/1911.09435'>TEINet</a>, an efficient video archiecture for video recognition, is accepted by AAAI 2020. </li>
    <li> 2019-07-23: Our proposed <a href='https://arxiv.org/abs/1908.04156'>LIP</a>, a general alternative to average or max pooling, is accepted by ICCV 2019. </li>
    <li> 2019-03-15: Two papers are accepted by CVPR 2019: one for group activity recognition and one for RGB-D transfer learning. </li>
    <li> 2018-08-19: One paper is accepted by ECCV 2018 and one by T-PAMI. </li>
    <li> 2018-04-01: <alert>I join <a href='https://www.nju.edu.cn/'>Nanjing University</a> as a faculty member at <a href='http://cs.nju.edu.cn/'>Department of Computer Science and Technology</a></alert>. </li>
    <li> 2017-11-28: We released a recent work on video architecture design for spatiotemporal feature learning. [ <a href='https://arxiv.org/abs/1711.09125'>arXiv</a> ] [ <a href='https://github.com/wanglimin/ARTNet'>Code</a> ].  </li>
    <li> 2017-09-08: We have released the TSN models learned in the <a href='http://yjxiong.me/others/kinetics_action/'>Kinetics</a> dataset. These models could be transferred well to the existing datasets for action recognition and detection [ <a href='http://yjxiong.me/others/kinetics_action/'>Link</a> ].  </li>
    <li> 2017-09-01: One paper is accepted by ICCV 2017 and one by IJCV. </li>
    <li> 2017-07-18: I am invited to give a talk at the <a href='https://sites.google.com/view/fvt2017/home'>Workshop on Frontiers of Video Technology-2017</a> [ <a href='fvt_slide.pdf'>Slide</a> ].
    <li> 2017-03-28: <alert>I am co-organizing the CVPR2017 workshop and challenge on Visual Understanding by Learning from Web Data</alert>. For more details, please see the <a href='http://www.vision.ee.ethz.ch/webvision/workshop.html'>workshop page</a> and <a href='https://competitions.codalab.org/competitions/16439'>challenge page</a>. </li>
    <li> 2017-02-28: Two papers are accepted by CVPR 2017. </li>
    <li> 2016-12-20: <alert>We release the code and models for SR-CNN paper</alert> [ <a href='https://github.com/yifita/action.sr_cnn'>Code</a> ]. </li> 
    <li> 2016-10-05: <alert>We release the code and models for Places2 scene recognition challenge</alert> [ <a href='https://arxiv.org/abs/1610.01119'>arXiv</a> ] [ <a href='https://github.com/wanglimin/MRCNN-Scene-Recognition'>Code</a> ]. </li>
    <li> 2016-08-03: <alert>Code and model of Temporal Segment Networks is released</alert> [ <a href='http://arxiv.org/abs/1608.00859'>arXiv</a> ] [ <a href='https://github.com/yjxiong/temporal-segment-networks'>Code</a> ]. </li>
    <li> 2016-07-15: One paper is accepted by ECCV 2016 and one by BMVC 2016. </li>
    <li> 2016-06-16: <alert>Our team secures the 1st place for untrimmed video classification at ActivityNet Challenge 2016</alert> [ <a href='http://activity-net.org/challenges/2016/program.html'>Result</a> ]. <br>
    Basically, our solution is based on our works of <a href='https://github.com/yjxiong/temporal-segment-networks'>Temporal Segment Networks</a> (TSN) and <a href = 'https://github.com/wanglimin/TDD'>Trajectory-pooled Deep-convolutional Descriptors</a> (TDD). </li>
    <li> 2016-03-01: Two papers are accepted by CVPR 2016. </li>
      <li> 2015-12-10: <alert>Our SIAT_MMLAB team secures the 2nd place for scene recognition at ILSVRC 2015 </alert> [ <a href='http://image-net.org/challenges/LSVRC/2015/results#scene'>Result</a> ].</li>
    <li> 2015-09-30: We rank 3rd for cultural event recognition on ChaLearn Looking at People challenge, at ICCV 2015. </li>
      <li> 2015-08-07: <alert>We release the Places205-VGGNet models</alert> [ <a href='https://github.com/wanglimin/Places205-VGGNet'>Link</a> ]. </li>
    <li> 2015-07-22: <alert>Code of Trajectory-Pooled Deep-onvolutional Descriptors (TDD) is released</alert> [ <a href='https://github.com/wanglimin/TDD'>Link</a> ]. </li>
		<li> 2015-07-15: <alert>Very deep two stream ConvNets are proposed for action recognition</alert> [ <a href='http://arxiv.org/abs/1507.02159'>Link</a> ]. </li>

    </ul>
  </div>
</div>
</div> -->

<div style="clear: both;">
<div class="section">
<!-- <h2 id="confpapers">Selected Publications[ <a href='publication.html'>Full List</a> ] </h2> -->
<h2 id="confpapers">图像处理、计算机视觉专业方向 </h2>
<div class="paper" id="GaoWW"><img class="paper" src="papers/Feature-Interaction.png" title="Feature Interaction Learning Network for Cross-Spectral Image Patch Matching" />
  <div> <strong>Feature Interaction Learning Network for Cross-Spectral Image Patch Matching[J].</strong><br />
  Yu, C., Liu, Y., Zhao, J., Wu, S., & Hu, Z.<br />
  in IEEE Transactions on Image Processing, 2023. (<strong>SCI 1区 TOP</strong>)<br />
  [ <a href='https://ieeexplore.ieee.org/abstract/document/10251126'>Paper</a> ] [ <a href='#'>Code</a> ] <br />
  <alert>Cross-Spectral Image Patch Matching.</alert>
  </div>
  <div class="spanner"></div>
  </div>

  <div class="paper" id="WuWWGW"><img class="paper" src="papers/COMBINING.png" title="COMBINING LOSS REWEIGHTING AND SAMPLE RESAMPLING FOR LONG-TAILED INSTANCE SEGMENTATION" />
  <div> <strong>COMBINING LOSS REWEIGHTING AND SAMPLE RESAMPLING FOR LONG-TAILED INSTANCE SEGMENTATION[C].</strong><br />
  Yaochi Zhao, Sen Chen, Qiong Chen, Zhuhua Hu*<br />
  in 2023 IEEE International Conference on Acoustics, Speech, and Signal Processing (<strong> ICASSP 2023</strong>),<br/> 4-6 June, Rhodes Island, Greece, 2023.<br />
  [ <a href='https://ieeexplore.ieee.org/abstract/document/10094303'>Paper</a> ] [ <a href=''>Code</a> ] <br />
  <!-- <alert>Obtaining STOA performance on datasets of Volleyball and Collective Activity.</alert> -->
  </div>
  <div class="spanner"></div>
  </div>
  
<div class="paper" id="TREG">
  <img class="paper" src="papers/ASHN.png" title="ASHN For Multi-Human Pose Estimation" />
<div> 
  <strong>ASHN For Multi-Human Pose Estimation [C].</strong><br />
  Pan Gao, Zhuhua Hu*.<br />
  The 6th Asian Conference on Artificial Intelligence Technology in Haikou, China (ACAIT 2022), Changzhou, China, December 10-12 2022. (已发表) (<strong>EI</strong>)  <br />
[ <a href='https://arxiv.org/abs/'>Paper</a> ] [ <a href=''>Code</a> ] <br />
<!-- <alert>Transformer for anchor-free tracking with obtaining SOTA performance</alert> -->
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="FCOT"><img class="paper" src="papers/ECI.png" title="ECI: Effective Channel Interaction for Person Search [C]." />

<div> <strong>ECI: Effective Channel Interaction for Person Search [C].</strong><br />
  Yongyi Ye, Zhuhua Hu*<br />
  The 6th Asian Conference on Artificial Intelligence Technology in Haikou, China (ACAIT 2022), Changzhou, China, December 10-12 2022. (已发表) (EI) <br />
[ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] <br />
<alert>Online learning of both classification and regression branch in a fully convolutional manner.</alert>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="PyMAF"><img class="paper" src="papers/A_Virtual_Try.png" title="A Virtual Try-on Model with Enhanced Feature Representation Capability" />
  <div> <strong> A Virtual Try-on Model with Enhanced Feature Representation Capability[C].</strong><br />
    Hui Ma, Zhuhua Hu*, Yan Zheng.<br />
The 6th Asian Conference on Artificial Intelligence Technology in Haikou, China (ACAIT 2022), Changzhou, China, December 10-12 2022. (已发表) (EI). <br />
  [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] [ <a href=''>Project Page</a> ]<br />
  <alert></alert>
  </div>
  <div class="spanner"></div>
  </div>

  <div class="paper" id="MGSampler"><img class="paper" src="papers/Multibranch_Feature_Difference_Learning_Network_for_Cross-Spectral_Image_Patch_Matching.png" title=">Multi-branch Feature Difference Learning Network for Cross-Spectral Image Patch Matching" />
    <div> <strong>Multi-branch Feature Difference Learning Network for Cross-Spectral Image Patch Matching[J].</strong><br />
      Yu, Chuang; Liu, yunpeng*; Li, Chenxi; Qi, Lin; Xia, Xin; Liu, Tianci; Hu, Zhuhua<br />
      in IEEE Transactions on Geoscience and Remote Sensing<strong>(SCI 1区，if:5.6, top)</strong>, 2022.<br />
    [ <a href='https://ieeexplore.ieee.org/abstract/document/9777946'>Paper</a> ] [ <a href=''>Code</a> ] <br />
    <alert>Multi-branch Feature Difference Learning Network.</alert>
    </div>
    <div class="spanner"></div>
    </div>

    <div class="paper" id="MGSampler"><img class="paper" src="papers/Group channel pruning and spatial attention distilling for object.png" title="Group channel pruning and spatial attention distilling for object.png" />
      <div> <strong>Group channel pruning and spatial attention distilling for object detection[J].</strong><br />
        Yun Chu, Pu Li, Yong Bai*, Zhuhua Hu, Yongqing Chen, Jiafeng Lu<br />
        in Applied Intelligence, 2022: 1-19.  <strong>(SCI 2区，IF:5.086)</strong><br />
      [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] <br />
      <!-- <alert>Multi-branch Feature Difference Learning Network.</alert> -->
      </div>
      <div class="spanner"></div>
      </div>
  
    <div class="paper" id="MGSampler"><img class="paper" src="papers/RF2Net Salient Object Detection Using Level Set Loss and Reverse Attention Fusion Feed Network.png" title=">RF2Net: Salient Object Detection Using Level Set Loss and Reverse Attention Fusion Feed Network" />
      <div> <strong>RF2Net: Salient Object Detection Using Level Set Loss and Reverse Attention Fusion Feed Network[C]</strong><br />
        Yutong Dai, Chong Zhang, Zhuhua Hu*, Yaochi Zhao<br />
        The 14th International Conference on Digital Image Processing (ICDIP 2022), Wuhan, China, 2022.5.20-23. <strong>(EI)</strong><br />
      [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] <br />
      <!-- <alert>Multi-branch Feature Difference Learning Network.</alert> -->
      </div>
      <div class="spanner"></div>
      </div>
  
      <div class="paper" id="MGSampler"><img class="paper" src="papers/unbalenced.png" title=">Dynamically balancing class losses in imbalanced deep learning" />
        <div> <strong>Dynamically balancing class losses in imbalanced deep learning[J].</strong><br />
          Yaochi Zhao, Shiguang Liu*, Zhuhua Hu.<br />
          Electronics Letters, 2022, 58(5): 203-206. <strong>(SCI)</strong><br />
        [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] <br />
        <!-- <alert>Multi-branch Feature Difference Learning Network.</alert> -->
        </div>
        <div class="spanner"></div>
        </div>
    
        <div class="paper" id="MGSampler"><img class="paper" src="papers/Focal.png" title="Focal learning on stranger for imbalanced image segmentation" />
          <div> <strong>Focal learning on stranger for imbalanced image segmentation[J].</strong><br />
            Yaochi Zhao, Shiguang Liu*, Zhuhua Hu*.<br />
            IET Image Processing, 2022, 16(5): 1305-1323. (SCI 4区， if：2.373)<br />
          [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] <br />
          <!-- <alert>Multi-branch Feature Difference Learning Network.</alert> -->
          </div>
          <div class="spanner"></div>
          </div>

          <div class="paper" id="MGSampler"><img class="paper" src="papers/DRLSE.png" title="" />
            <div> <strong>基于改进DRLSE的运动目标分割方法[J].</strong><br />
              胡祝华,赵瑶池*,程杰仁,等.<br />
              [11]浙江大学学报：工学版，2014,48(8):1488~1495.（EI源刊）.<br />
            [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] <br />
            <!-- <alert>Multi-branch Feature Difference Learning Network.</alert> -->
            </div>
            <div class="spanner"></div>
            </div>

            <!-- <div class="paper" id="MGSampler"><img class="paper" src="papers/DRLSE.png" title="" />
              <div> <strong>Moving object detection method with temporal and spatial variation based on multi-info fusion[C].</strong><br />
                Zhao Yaochi, Hu Zhuhua*, Yang X, Bai Y.<br />
                [12]10th International Conference on Intelligent Computing, ICIC 2014; Taiyuan, China: Springer Verlag; 2014. pp. 387-397.（EI收录）<br />
              [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] <br />
              <alert>Multi-branch Feature Difference Learning Network.</alert>
              </div>
              <div class="spanner"></div>
              </div>
   -->
<h2 id="confpapers">VSLAM机器人方向</h2>
<!-- [ <a href='publication.html'>Full List</a> ]  -->
<div class="paper" id="CPD"><img class="paper" src="papers/CBAM-SLAM_A_Semantic_SLAM_Based_on_Attention_Module_in_Dynamic_Environment.png" title="CBAM-SLAM: A semantic SLAM based on attention module in dynamic environment" />
<div> <strong>CBAM-SLAM: A semantic SLAM based on attention module in dynamic environment [C]</strong><br />
  Yuexin Fu, Bing Han, Zhuhua Hu*, Xiuqiang Shen, Yaochi Zhao.<br />
  The 6th Asian Conference on Artificial Intelligence Technology (ACAIT 2022), Changzhou, China, December 10-12 2022. (<strong>EI</strong>) <br />
[ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] <br />
<!-- <alert>We propose a weakly supervised video representation learning framework from text information.</alert> -->
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="PyMAF"><img class="paper" src="papers/A Closed-loop Detection Algorithm for Online Updating of Bag-of-Words Model.png" title="A Closed-loop Detection Algorithm for Online Updating of Bag-of-Words Model [C]" />
<div> <strong>A Closed-loop Detection Algorithm for Online Updating of Bag-of-Words Model [C]</strong><br />
  Xiuqiang Shen, Lihang Chen, Zhuhua Hu*, Yuexin Fu, Hao Qi, Yunfeng Xiang, Jiaqi Wu.<br />
  The 9th International Conference on Computing and Data Engineering (ICCDE 2023), Haikou, China, January 6-8, 2023. (<strong>EI</strong>)<br />
[ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] [ <a href=''>Project Page</a> ]<br />
<alert></alert>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="MGSampler"><img class="paper" src="papers/ATY-SLAM A Visual Semantic SLAM for Dynamic Indoor Environments.png" title=">" />
<div> <strong>ATY-SLAM: A Visual Semantic SLAM for Dynamic Indoor Environments [C]</strong><br />
  Hao Qi, Zhuhua Hu*, Yunfeng Xiang, Dupeng Cai, and Yaochi Zhao<br />
  the 19th International Conference on Intelligent Computing, August 10-13, 2023, Zhengzhou, China. (<strong>(EI, CCF C类)</strong>) <br />
[ <a href=''>Paper</a> ] [ <a href=''>Code (soon)</a> ] <br />
<alert></alert>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="MultiSports"><img class="paper" src="papers/AGAM-SLAM An Adaptive Dynamic Scene Semantic SLAM Method Based on GAM.png" title="AGAM-SLAM An Adaptive Dynamic Scene Semantic SLAM Method Based on GAM.png" />
<div> <strong>AGAM-SLAM: An Adaptive Dynamic Scene Semantic SLAM Method Based on GAM [C]</strong><br />
  Dupeng Cai, Zhuhua Hu*, Ruoqing Li, Hao Qi, Yunfeng Xiang, and Yaochi Zhao<br />
  the 19th International Conference on Intelligent Computing, August 10-13, 2023, Zhengzhou, China.(<strong>EI, CCF C类</strong>) <br />
[ <a href=''>Paper</a> ] [ <a href=''>Data</a> ] [ <a href=''>Code</a> ] <br />
<alert></alert>
</div>
<div class="spanner"></div>
</div>

<!-- <div class="paper" id="TAM"><img class="paper" src="" title="Energy-and-Time-Aware Data Acquisition for Mobile Robots Using Mixed Cognition Particle Swarm Optimization [J]." />
<div> <strong>Energy-and-Time-Aware Data Acquisition for Mobile Robots Using Mixed Cognition Particle Swarm Optimization [J].</strong><br />
  Mingshan Xie, Yong Bai*, Mengxing Huang,Yanfang Deng, and Zhuhua Hu.<br />
  [5]IEEE Internet of Things Journal, 2020, 7(8), 7734-7750. (<strong>SCI, IF: 9.515, 中科院1区，Top</strong>) <br />
[ <a href='https://arxiv.org/abs/2005.06803'>Paper</a> ] [ <a href='https://github.com/liu-zhy/TANet'>Code</a> ] <br />
<alert>Temporal adaptive module of self attention + dynamic filtering for video recognition.</alert>
</div>
<div class="spanner"></div>
</div> -->
<h2 id="confpapers">遥感，船舶小目标、模糊目标和异常行为检测方向</h2>
<div class="paper" id="RTD"><img class="paper" src="papers/Ship Detection and Recognition Based on Improved YOLOv7.png" title="" />
<div> <strong>Ship Detection and Recognition Based on Improved YOLOv7[J].</strong><br />
  Wu W, Li X, Hu Z, et al.<br />
  [1]Computers, Materials & Continua, 2023, 76(1). (<strong>中科院SCI 4区</strong>) <br />
[ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] <br />
<alert></alert>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="RTD"><img class="paper" src="papers/Infrared small target detection based on multiscale local contrast.png" title="" />
  <div> <strong>Infrared Small Target Detection Based on Multiscale Local Contrast Learning Networks[J].</strong><br />
    Yu Chuang, Liu Yunpeng*,Wu Shuhang, Hu Zhuhua, Xia Xin, Lan Deyan, Liu Xin.<br />
    Infrared Physics & Technology, 2022: 104107. (<strong>中科院2区 SCI，if：2.638</strong>) <br />
  [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] <br />
  <alert></alert>
  </div>
  <div class="spanner"></div>
  </div>

  <div class="paper" id="RTD"><img class="paper" src="papers/Pay_Attention_to_Local_Contrast_Learning_Networks_for_Infrared_Small_Target_Detection.png" title="" />
    <div> <strong>Pay Attention to Local Contrast Learning Networks for Infrared Small Target Detection[J].</strong><br />
      Chuang Yu, Yunpeng Liu, Shuhang Wu, Xin Xia, Zhuhua Hu, Deyan Lan and Xin Liu,.<br />
      IEEE GEOSCIENCE AND REMOTE SENSING LETTERS, 2022.(<strong>中科院SCI 2区，if 3.966</strong>) <br />
    [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] <br />
    <alert></alert>
    </div>
    <div class="spanner"></div>
    </div>

    <h2 id="confpapers">智慧农业-水产养殖方向</h2>

    <!-- <div class="paper" id="TDN"><img class="paper" src="papers/tdn_20.png" title="" />
      <div> <strong>Shengsen Peng, Microbial Colony Detection Based on Deep Learning [J]</strong><br />
        Fan Yang, Yongjie Zhong *, Hui Yang, Yi Wan, Zhuhua Hu<br />
        [1]Applied Sciences-Basel, 2023.08.  (<strong>中科院SCI 3区</strong>)<br />
      [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] <br />
      <alert></alert>
      </div>
      <div class="spanner"></div>
      </div> -->
      <div class="paper" id="TDN"><img class="paper" src="papers/An intelligent measurement scheme for basic characters of fish in smart.png" title="" />
        <div> <strong>An Intelligent Measurement Scheme for Basic Characters of Fish in Smart Aquaculture</strong><br />
          Chuang Yu, Zhuhua Hu*, Bing Han, Yutong Dai, Yaochi Zhao, Yingjun Deng<br />
          Computers and Electronics in Agriculture, 2023.  (<strong>中科院SCI 1区 top，if 6.757</strong>)<br />
    [ <a href='#'>Paper</a> ] [ <a href='#'>Code</a> ] <br />
    <alert></alert>
    </div>
    <div class="spanner"></div>
    </div>

    <div class="paper" id="TDN"><img class="paper" src="papers/Mask_LaC R-CNN for measuring morphological features of fish.png" title="" />
      <div> <strong>Mask_LaC R-CNN for measuring morphological features of fish</strong><br />
        Bing Han, Zhuhua Hu*, Zhengwei Su, Xueru Bai, Shuzhuang Yin, Jian Luo, Yaochi Zhao.<br />
        [4]Measurement, 2022, 203: 111859. (<strong>中科院SCI 2区，IF:5.131</strong>)<br />
      [ <a href='#'>Paper</a> ] [ <a href='#'>Code</a> ] <br />
      <alert></alert>
      </div>
      <div class="spanner"></div>
      </div>

      <div class="paper" id="TDN"><img class="paper" src="papers/DA-Bi-SRU for Water Quality Prediction in Smart Mariculture.png" title="" />
        <div> <strong>DA-Bi-SRU for Water Quality Prediction in Smart Mariculture [J]</strong><br />
          Zijie Chen, Zhuhua Hu*, Lewei Xu, Yaochi Zhao*, Xiaoyi Zhou<br />
          Computers and Electronics in Agriculture, 2022, 200: 107219. (<strong>中科院SCI 1区，top，if 6.757</strong>)<br />
        [ <a href='#'>Paper</a> ] [ <a href='#'>Code</a> ] <br />
        <alert></alert>
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper" id="TDN"><img class="paper" src="papers/Remote Sensing Image Segmentation of Mariculture Cage Using Ensemble Learning Strategy.png" title="" />
          <div> <strong>Remote Sensing Image Segmentation of Mariculture Cage Using Ensemble Learning Strategy[J].</strong><br />
            Lewei Xu, Zhuhua Hu*, Cong Zhang, Wei Wu.<br />
          Applied Sciences-Basel, 2022, 12(16): 8234. (<strong>中科院SCI 3区，if 2.838</strong>)<br />
          [ <a href='#'>Paper</a> ] [ <a href='#'>Code</a> ] <br />
          <alert></alert>
          </div>
          <div class="spanner"></div>
          </div>
  

          <div class="paper" id="TDN"><img class="paper" src="papers/Precise segmentation of remote sensing cage images based on SegNet and voting mechanism.png" title="" />
            <div> <strong>Precise segmentation of remote sensing cage images based on SegNet and voting mechanism[J]</strong><br />
              Yu, Chuang; Liu, yunpeng; Xia, Xin*; Hu, Zhuhua*; Fu, Shengpeng.<br />
              Applied Engineering in Agriculture, 2022：0.  (<strong>SCI, 4区, 0.985</strong>)<br />
            [ <a href='#'>Paper</a> ] [ <a href='#'>Code</a> ] <br />
            <alert></alert>
            </div>
            <div class="spanner"></div>
            </div>
    
    

            <div class="paper" id="TDN"><img class="paper" src="papers/Precise segmentation and measurement of inclined fish’s features based on U-net and fish morphological characteristics.png" title="" />
              <div> <strong>Precise segmentation and measurement of inclined fish’s features based on U-net and fish morphological characteristics[J]. </strong><br />
                Chuang Yu, Yunpeng Liu*, Zhuhua Hu, Xin Xia<br />
                Applied Engineering in Agriculture, 2022, 38(1): 37-48. (<strong>SCI, 中科院4区，if: 0.985</strong>)<br />
              [ <a href='#'>Paper</a> ] [ <a href='#'>Code</a> ] <br />
              <alert></alert>
              </div>
              <div class="spanner"></div>
              </div>

<div class="paper" id="TDN"><img class="paper" src="papers/Accurate segmentation of remote sensing cages based on U-Net and voting mechanism.png" title="" />
  <div> <strong>Accurate segmentation of remote sensing cages based on U-Net and voting mechanism [C] </strong><br />
    Yu Chuang, Liu Yunpeng, Hu Zhuhua*, Xia Xin*<br />
    “智能感知与跨域协同”体系研究前沿论坛会议论文集, 中国 长春, 2021.09.25号 – 27号, 2022. (EI)   (<strong>EI</strong>)<br />
  [ <a href='#'>Paper</a> ] [ <a href='#'>Code</a> ] <br />
  <alert></alert>
  </div>
  <div class="spanner"></div>
  </div>

  <div class="paper" id="TDN"><img class="paper" src="papers/A real time video object tracking method for fish.png" title="" />
    <div> <strong>A real time video object tracking method for fish [C].</strong><br />
      Longqin Gong, Zhuhua Hu*, Xaioyi Zhou.<br />
      the 6th International Conference on Machine Learning and Soft Computing (ICMLSC 2022), Haikou, China, January 15-17, 2022. (<strong>EI</strong>)<br />
    [ <a href='#'>Paper</a> ] [ <a href='#'>Code</a> ] <br />
    <alert></alert>
    </div>
    <div class="spanner"></div>
    </div>
  
  
    <div class="paper" id="TDN"><img class="paper" src="papers/Abnormal Behavior Recognition of Underwater Fish Body Based C3D.png" title="" />
      <div> <strong>Abnormal Behavior Recognition of Underwater Fish Body Based on C3D Model [C].</strong><br />
        Zhuhua Hu, Xianghui Li, Xinyu Xie, Yaochi Zhao*. <br />
        the 6th International Conference on Machine Learning and Soft Computing (ICMLSC 2022), Haikou, China, January 15-17, 2022. (<strong>EI</strong>)<br />
      [ <a href='#'>Paper</a> ] [ <a href='#'>Code</a> ] <br />
      <alert></alert>
      </div>
      <div class="spanner"></div>
      </div>

      <div class="paper" id="TDN"><img class="paper" src="papers/A Few Samples Underwater Fish Tracking Method.png" title="" />
        <div> <strong>A few samples underwater fish tracking method based on semi-supervised and attention mechanism</strong><br />
          Longqin Gong, Zhuhua Hu*, Xaioyi Zhou. <br />
          2022 6th International Conference on Robotics, Control and Automation (ICRCA), Xiamen, China, February 26-28, Pages 18-22, 2022. (<strong>EI</strong>)<br />
        [ <a href='#'>Paper</a> ] [ <a href='#'>Code</a> ] <br />
        <alert></alert>
        </div>
        <div class="spanner"></div>
        </div>
      
         
    
<h2 id="confpapers">遥感，船舶小目标、模糊目标和异常行为检测方向</h2>    


<div class="paper" id="TDN"><img class="paper" src="papers/ShipDetection and Recognition Based on Improved YOLOv7.png" title="" />
  <div> <strong>hip Detection and Recognition Based on Improved YOLOv7[J].</strong><br />
    Wu W, Li X, Hu Z, et al. <br />
    [1]Computers, Materials & Continua, 2023, 76(1).  (<strong>中科院SCI 4区</strong>)<br />
  [ <a href='#'>Paper</a> ] [ <a href='#'>Code</a> ] <br />
  <alert></alert>
  </div>
  <div class="spanner"></div>

  </div>
  <div class="paper" id="TDN"><img class="paper" src="papers/Infraredsmall target detection based on multiscale local contrast.png" title="" />
    <div> <strong>Infrared Small Target Detection Based on Multiscale Local Contrast Learning Networks[J].</strong><br />
      Yu Chuang, Liu Yunpeng*,Wu Shuhang, Hu Zhuhua, Xia Xin, Lan Deyan, Liu Xin. <br />
      Infrared Physics & Technology, 2022: 104107. (<strong>中科院2区 SCI，if：2.638</strong>)<br />
    [ <a href='#'>Paper</a> ] [ <a href='#'>Code</a> ] <br />
    <alert></alert>
    </div>
    <div class="spanner"></div>
    </div>

    <div class="paper" id="TDN"><img class="paper" src="papers/Pay_Attention_to_Local_Contrast_Learning_Networks_for_Infrared_Small_Target_Detection.png" title="" />
      <div> <strong>Pay Attention to Local Contrast Learning Networks for Infrared Small Target Detection[J].</strong><br />
        Chuang Yu, Yunpeng Liu, Shuhang Wu, Xin Xia, Zhuhua Hu, Deyan Lan and Xin Liu <br />
        IEEE GEOSCIENCE AND REMOTE SENSING LETTERS, 2022.  (<strong>中科院SCI 2区，if 3.966</strong>)<br />
      [ <a href='#'>Paper</a> ] [ <a href='#'>Code</a> ] <br />
      <alert></alert>
      </div>
      <div class="spanner"></div>
      </div>

<h2 id="confpapers">频谱感知与移动通信方向</h2>    
<div class="paper" id="TEA"><img class="paper" src="papers/bcn_20.png" title="" />
<div> <strong>SNR Estimation Method based on SRS and DINet</strong><br />
  Guohua Yao and Zhuhua Hu*. <br />
  In Proceedings of the 2023 15th International Conference on Computer Modeling and Simulation (ICCMS '23). ACM, Dalian, China, June 16-18 2023, PP. 218–224. (<strong>EI</strong>) <br />
[ <a href='https://doi.org/10.1145/3608251.3608262'>Paper</a> ] [ <a href='https://mp.weixin.qq.com/s/BajtO_bR4dDzyPRvzD81hQ'>会议回顾</a> ] <br />
<alert></alert>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="TEA"><img class="paper" src="papers/TFF_aDCNN_A_Pre-Trained_Base_Model_for_Intelligent_Wideband_Spectrum_Sensing.png" title="" />
<div> <strong>TFF_aDCNN: A Pre-trained Base Model for Intelligent Wideband Spectrum Sensing [J]</strong><br />
  Xianghui Li, Zhuhua Hu*, Chong Shen, Huaming Wu*, Yaochi Zhao <br />
   IEEE Transactions on Vehicular Technology, 2023. (<strong>SCI 中科院2区, top, IF 6.239</strong>) <br />
[ <a href=''>Paper</a> ] [ <a href=''>会议回顾</a> ] <br />
<alert></alert>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="TEA"><img class="paper" src="papers/Design and Implementation of Shared Memory for Turbo and LDPC Code Interleaver.png" title="" />
  <div> <strong>Design and Implementation of Shared Memory for Turbo and LDPC Code Interleaver[J]</strong><br />
    Kejia Huo, Zhuhua Hu*, Dake Liu.<br />
    Wireless Communications & Mobile Computing, 2022.(<strong>SCI 4区</strong>) <br />
    基金：Grant No. 61963012 and Grant no. 62161010 <br />
  [ <a href=''>Paper</a> ] [ <a href=''>会议回顾</a> ] <br />
  <alert></alert>
  </div>
  <div class="spanner"></div>
  </div>
  
  <h2 id="confpapers">智慧农业-病虫害智能监控方向</h2>    

<div class="paper" id="TEA"><img class="paper" src="papers/A YOLOv7 incorporating the Adan optimizer based corn pests identification method.png" />
<div> <strong>A YOLOv7 incorporating the Adan optimizer based corn pests identification method [J].</strong><br />
  Chong Zhang, Zhuhua Hu*, Lewei Xu, Yaochi Zhao.<br />
  [1]Frontiers in Plant Science, 2023.(<strong>SCI 1区 TOP, 已发表</strong>)<br />
[ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] <br />
<alert></alert>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="MOC-Dector"><img class="paper" src="papers/Intelligent Detection of Mango Disease Spores Based on Mask Scoring R-CNN.png" title="Actions as Moving Points" />
<div> <strong> Intelligent Detection of Mango Disease Spores Based on Mask Scoring R-CNN [C].</strong><br />
  Xinyu Xie, Jiaying Wang, Zhuhua Hu*, Yaochi Zhao*. <br />
  the 5th Asian Conference on Artificial Intelligence Technology in Haikou,China (ACAIT 2021), Haikou,China, October 29-31 2021. (<strong>EI</strong>)<br />
[ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] <br />
<alert></alert>
</div>
<div class="spanner"></div>
</div>
<div class="paper" id="MOC-Dector"><img class="paper" src="papers/" title="Actions as Moving Points" />
  <div> <strong> Recognition of Pyralidae Insects with Unmanned Monitoring Robot Based on Histogram Reverse Mapping and Invariant Moment[C]</strong><br />
    Zhuhua Hu, Boyi Liu*, Yaochi Zhao, Mengxing Huang, Yong Bai and Fusheng Lin.<br />
    2018 IEEE International Conference on Advanced Manufacturing (ICAM). IEEE, 2018: 407-410. (<strong>EI</strong>)<br />
  [ <a href=''>Paper</a> ] [ <a href=''>Code</a> ] <br />
  <alert></alert>
</div>
<div class="spanner"></div>
</div>

  
<!-- 
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="DSN"><img class="paper" src="papers/DSN_20.png" title="Dynamic Sampling Networks for Efficient Action Recognition in Videos" />
<div> <strong>Dynamic Sampling Networks for Efficient Action Recognition in Videos</strong><br />
Y. Zheng, Z. Liu, T. Lu, L. Wang<br />
in IEEE Transactions on Image Processing (<strong>TIP</strong>), 2020. <br />
[ <a href='https://arxiv.org/abs/2006.15560'>Paper</a> ] <br />
<alert>A dynamic version of TSN for efficient action recognition.</alert>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="Liu"><img class="paper" src="papers/TEI_20.png" title="TEINet: Towards an Efficient Architecture for Video Recognition" />
<div> <strong>TEINet: Towards an Efficient Architecture for Video Recognition</strong><br />
Z. Liu, D. Luo, Y. Wang, L. Wang, Y. Tai, C. Wang, J. Li, F. Huang, T. Lu <br />
in AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2020. <br />
[ <a href='https://arxiv.org/abs/1911.09435'>Paper</a> ] <br />
<alert>An efficient architecture for video recognition based on 2D CNN.</alert>
</div>
<div class="spanner"></div>
</div>


-->


<div style="clear: both;">
<div class="section">
<h2 id="confpapers">其它</h2>
<div class="paper">
<h3 id="confpapers">教改类</h2>   
<ul>
<li>彭金莲, 胡祝华, 郑兆华, 等. 网络工程专业“3+1”模块化课程体系的创新研究[J]. 海南大学学报(自然科学版),2013,31(01):74-79.</li>
<li>赵瑶池, 胡祝华*, 陈明锐, 等. 以计算思维为导向的大学 “计算机基础” 课程教学改革研究[J]. 海南大学学报: 自然科学版, 2014, 32(4): 383-388.</li>
</ul>
<h3 id="confpapers">教材</h2>   
<ul>
<li>邱钊、赵瑶池、胡祝华等，《Java程序设计实用教程》， 2023.</li>
<li>张红、胡祝华主编， 《Java程序设计案例教程》,高等教育出版社，2019年.8月。（“十三五”国家级规划教材），ISBN：978-7-04-051472-8，CIP核准字：2019037891。 46.2万字。</li>
<li>十二五规划教材 《C语言程序设计实验教程》 中国农业出版社 完成。2012-07-15出版 isbn：978-7-109-16866-4 编写第4、9章（总33.2万字，6.9万字）。</li>
<li>十二五规划教材 《C语言程序设计（第二版）》 中国农业出版社 完成。2012-07-15出版 isbn：978-7-109-16850-3 编写第四章（总54.2万字，2.6万字）。</li>
</ul>
<h3 id="confpapers">专著</h2>   
<ul>
<li>周星，陈敏，白勇，胡祝华，谭毓银，《下一代互联网新技术理论与实践》，科学出版社，2022.11. ISBN: 978-7-03-072720-6</li>
<li>王振龙，胡祝华.《物联网与通信技术的理论与实践探索》. 电子科技大学出版社. 2019.10, ISBN: 978-7-5647-5451-8.CIP核准号：2018009134，29.8万。</li>
<li>胡祝华、赵瑶池著.《计算机视觉与机器学习技术在智慧农业中的应用研究》， 哈尔滨工业大学出版社，ISBN: 978-7-5603-8053-7, CIP核准号：2019045734，2019.3, 39.1万字.</li>
<li>黄霞（主编），胡祝华（副主编）.《现代软件架构与模式解析研究》，哈尔滨工业大学出版社，2019.04，ISBN: 978-7-5603-8107-7。CIP核准字:2019067884，2019.4。35.5万字</li>
<li>Zhuhua Hu. “Agricultural Robots - Fundamentals and Applications”, 2019.1. ISBN: 978-1-78984-934-9. 著作章节：Agricultural Robot for Intelligent Detection of Pyralidae Insects. IntechOpen, London, UK.
  Zhou J, Zhang B. Agricultural Robots-Fundamentals and Applications [J]. 2019. Hu Z, Liu B, Zhao Y. Agricultural Robot for Intelligent Detection of Pyralidae Insects [J]. 2018.</li>
<li>白勇，胡祝华编著. 《GNU Radio软件无线电技术》. 科学出版社. 256千字，2017.01, ISBN: 978-7-0305-0757-0。CIP核准号：2016279409</li>

</ul>
</div>
</div>
</div>


<!-- 
</ul>
<div class="spanner"></div>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Academic Service</h2>
<div class="paper">
<strong>Journal Reviewer</strong> <br>
<p> IEEE Transactions on Pattern Analysis and Machine Intelligence </p>
<p> IEEE Transactions on Image Processing </p>
<p> IEEE Transactions on Multimedia </p>
<p> IEEE Transactions on Circuits and Systems for Video Technology </p>
<p> Pattern Recognition </p>
<p> Pattern Recognition Letter </p>
<p> Image and Vision Computing </p>
<p> Computer Vision and Image Understanding </p>
<br />
<strong>Conference Reviewer</strong> <br>
<p> IEEE Conference on Computer Vision and Pattern Recognition, 2017 </p>
<p> IEEE International Conference on Automatic Face and Gesture Recognition, 2017 </p>
<p> European Conference on Computer Vision, 2016 </p>
<p> Asian Conference on Computer Vision, 2016 </p>
<p> International Conference on Pattern Recognition, 2016 </p>
<div class="spanner"></div>
</div>
</div>
</div> -->

<!--
<div style="clear: both;">
<div class="section"><h2>Courses</h2>
<div class="paper">
<strong>Teaching Assistant for</strong> <br>
<p> ENGG2430, Probability and Statistics for Engineers ( Spring 2015 ). </p>
<p> ENGG1100, Introduction to Engineering Design ( Autumn 2014 ). </p>
<br />
<strong>Graduate Courses</strong> <br>
<p> SEEM 5121, Numerical Optimization ( Spring 2015 ) [ <a href='http://www1.se.cuhk.edu.hk/~sqma/SEEM5121'>Webpage</a> ]. </p>
<p> IERG 6130, Advanced Topics on Machine Learning and Probabilistic Inference ( Spring 2015 ) [ <a href='http://lindahua.github.io/MLPI/'>Webpage</a> ]. </p>
<p> ELEG 5481, Signal Processing Optimization Techniques ( Spring 2013 ) [ <a href='http://dsp.ee.cuhk.edu.hk/eleg5481/'>Webpage</a> ]. </p>
<p> IERG 6210, Advanced Topic in Information Processing (Spring 2012 ) [ <a href='https://course.ie.cuhk.edu.hk/~ierg6210/'>Webpage</a> ]. </p>
<p> CSCI 5160, Spectral Algorithm ( Spring 2012) [ <a href='http://www.cse.cuhk.edu.hk/~chi/csc5160/'>Webpage</a> ]. </p>
<p> SEEM 5520, Optimization I ( Autumn 2011 ) [ <a href='http://www1.se.cuhk.edu.hk/~manchoso/1112/seem5520/'>Webpage</a> ]. </p>
<p> IERG 5154, Information Theory ( Autumn 2011 ) [ <a href="http://ieg5154.pbworks.com/w/page/8690729/FrontPage">Webpage</a> ]. </p>
</div>
</div>
</div>
-->

<!-- <div style="clear: both;">
<div class="section"><h2>Friends</h2>
<div class="paper">
<a href='http://www.vision.ee.ethz.ch/~liwenw/'>Wen Li</a> (ETH), <a href='https://ait.ethz.ch/people/song/'>Jie Song</a> (ETH), <a href='http://guoshengcv.github.io/'>Sheng Guo</a> (Malong), <a href='http://www.whuang.org/'>Weilin Huang</a> (Malong), <a href='http://zbwglory.github.io/'>Bowen Zhang</a> (USC), <a href='http://wangzheallen.github.io/'>Zhe Wang</a> (UCI), <a href='http://liwei.ml/'>Wei Li</a> (Google), <a href='http://personal.ie.cuhk.edu.hk/~xy012/'>Yuanjun Xiong</a> (Amazon), <a href='http://pengxj.github.io/'>Xiaojiang Peng</a> (SIAT), <a href='https://zhuoweic.github.io/'>Zhuowei Cai</a> (Google), <a href='https://scholar.google.com.sg/citations?user=fPwq28oAAAAJ&hl=en'>Xingxing Wang</a> (NTU)
</div>
</div>
</div> -->

<div style="clear:both;">
<p align="right"><font size="5">Last Updated on 5th October, 2023</a></font></p>
<p align="right"><font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font></p>
</div>

</body>
</html>
